{"pages":[{"title":"Water Books","text":"California Water Books -Gil Masters, \"Renewable Energy\"","tags":"Reading","url":"pages/books/"},{"title":"About","text":"Biography Gage was the Chief Researcher , Vice-President, and Director of the Science Office for Sun Microsystems 1 , Inc and a partner in the venture capital firm Kleiner, Perkins, Caufield and Byers. Gage attended the University of California, Berkeley, the Harvard Kennedy School of Government, and the Harvard Business School. He directed the annual JavaOne conference, bringing over 20,000 programmers annually to a 5-day, 450-course intensive collaboration on developing the Java programming language. Gage has served on scientific advisory panels for the US National Research Council, the US National Academy of Sciences , and a variety of national and international organizations. He served on the Markle Task Force on National Security founded in response to 9/11, whose reports guided the reorganization of US intelligence and security agencies post 9/11 [&#94;2]. He served on the US National Academy of Sciences Committee on Scientific Communication and National Security , whose report, \"Beyond Fortress America: National Security Controls on Science and Technology in a Globalized World\" was published by NAS in 2009 2 . He has been a member of the Board of Regents of the US National Library of Medicine , the Board of Trustees of Fermi National Laboratory , the External Advisory Council for the World Bank , the Board of Trustees of the Internet Society (ISOC) , the Board of Advisors of the US Institute of Peace , the Dean's Council of the Harvard Kennedy School of Public Policy , the Board of the Pardee RAND Graduate School of Public Policy , and the Dean's Council of the UC Berkeley Goldman School of Public Policy . He created NetDay , the first mass movement organized on the Internet, which mobilized hundreds of thousands of parents and engineers to install Ethernet networks in US schools. In 1995, using meter-accurate maps of 140,000 US K-12 schools, he pioneered the use of the Internet for completely decentralized self-organized mass movements. NetDays spread to Korea, Malaysia, the UK, France and other countries. Gage received the ACM President's Special Award in 1998 for connecting millions of children to the Internet. He was a fellow of the Harvard Kennedy School of Government's Shorenstein Center , and taught Technology, Media, and Governance in Fall, 2000 at the Harvard Kennedy School to a class of 90 students. He is on the Board of the Tegla Loroupe Peace Foundation of Kenya , working to bring peace and economic development to the nomadic tribes of the Rift Valley of Kenya, and on the board of the Human Needs Project, building a water, sanitation, and information technology center in the Kibera slum in Nairobi, Kenya. He is on the Board of Relief International , the Oxford James Martin School for the 21 st Century , the University of California, Berkeley Goldman School of Public Policy, the Library of University of California, Berkeley, the World-Wide Web Foundation , and the San Francisco Gray Area Foundation for the Arts. He recently joined the board of the Open Source Voting Technology Institute . He is a member of the 2018 City of Berkeley Vision2050 Commission , established to prepare a long-range strategic plan based on modeling water, power, health, transportation, and financial systems of Berkeley over the next thirty years. The Commission is charged with preparing a $1 billion dollar bonding plan for infrastructure investment in the next year. He is on the Advisory Board of Liquid Robotics, creator of autonomous ocean data acquisition vehicles powered by wave action, recently acquired by Boeing. He is certified by the State of California as a Water Treatment Operator , Level II, a Water Distribution Operator , Level II, a Waste Water Treatment Operator , Level III, and maintains a real-time data acquisition system for the water, waste-water, and water distribution system of the Kibera Town Centre in the center of the Kibera slum in Nairobi, Kenya. He completed Parts One, Two, and Three of the UC Berkeley Foundations of Data Science EdX course. Gage lives in Berkeley with his wife Linda Schacht, former UC Berkeley Journalism faculty member, and has two children, Peter and Kate. Sun Microsystems. ↩ Beyond Fortress America ↩","tags":"About","url":"pages/About/"},{"title":"Deep Learning","text":"Jeff Dean @JeffDean ai.google/research/people/jeff David Patterson, Carlos Sequin, David Culler, ML for robotic control 90 ML paper / year","tags":"Cities","url":"articles/2019/Jan/deep-learning/"},{"title":"\"JNB tests\"","text":"Lab 5: Song Classification, Part 2 Welcome to Lab 5! We'll pick off where we left off and continue to build a song classifer using k-nearest neighbors. Please complete lab 4 before starting lab 5. Lab 4 is part 1 of the investigation. Lab 5 is part 2 of the investigation. You will build a classifier that guesses whether a song is hip-hop or country, using only the number of times that each word appears in a song's lyrics. By the end of the project, you should know how to: Clean and organize a dataset used to test a machine learning model Build a k-nearest neighbors classifier Test a classifier on data Advice. Develop your answers incrementally. To perform a complicated table manipulation, break it up into steps, perform each step on a different line, give a new name to each result, and check that each intermediate result is what you expect. You can add any additional names or functions you want to the provided cells. To get started, load datascience , numpy , plots , and okgrade . In [42]: # Run this cell to set up the notebook, but please don't change it. import numpy as np import math from datascience import * # These lines set up the plotting functionality and formatting. import matplotlib matplotlib . use ( 'Agg' , warn = False ) % matplotlib inline import matplotlib.pyplot as plots plots . style . use ( 'fivethirtyeight' ) import warnings warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) warnings . simplefilter ( 'ignore' , UserWarning ) # These lines load the tests. from okgrade import grade Overview: Recap In lab 4, we completed the following tasks: In section 1, we explored the dataset and split the dataset into training data and test data. In section 2, we ran through a guided example of the k-Nearest Neightbors (k-NN) classification algorithm. If you do not remember lab 4, we highly recommend you go back and review it now. It will help you for this lab. In lab 5, we are going to complete the following tasks: Identify some features. Define a classifier function using your features and the training set. Evaluate its performance (the proportion of correct classifications) on the test set. Run the cell below to set up the lab. In [43]: lyrics = Table . read_table ( 'lyrics.csv' ) training_proportion = 11 / 16 num_songs = lyrics . num_rows num_train = int ( num_songs * training_proportion ) num_valid = num_songs - num_train train_lyrics = lyrics . take ( np . arange ( num_train )) test_lyrics = lyrics . take ( np . arange ( num_train , num_songs )) def most_common ( label , table ): return table . group ( label ) . sort ( 'count' , descending = True ) . column ( label ) . item ( 0 ) 1. Features Now, we're going to extend our classifier from lab 4 to consider more than two features at a time. Euclidean distance still makes sense with more than two features. For n different features, we compute the difference between corresponding feature values for two songs, square each of the n differences, sum up the resulting numbers, and take the square root of the sum. Question 1.1 Write a function to compute the Euclidean distance between two arrays of features of arbitrary (but equal) length. Use it to compute the distance between the first song in the training set and the first song in the test set, using all of the features . (Remember that the title, artist, and genre of the songs are not features.) Note: To convert row objects to arrays, use np.array . For example, if t was a table, np.array(t.row(0)) converts row 0 of t into an array. In [44]: feature_names = np . array ( train_lyrics . column_labels [ 3 : len ( train_lyrics . column_labels )]) feature_names , #type(features) Out[44]: (array(['i', 'the', 'you', ..., 'pe', 'gee', 'colleg'], dtype='<U13'),) In [45]: features0 = train_lyrics . row ( 0 )[ 3 :] In [46]: def distance ( features1 , features2 ): \"\"\"The Euclidean distance between two arrays of feature values.\"\"\" return np . sqrt ( sum (( np . array ( features1 ) - np . array ( features2 )) ** 2 )) distance_first_to_first = distance ( train_lyrics . row ( 0 )[ 3 :], test_lyrics . row ( 0 )[ 3 :]) distance_first_to_first Out[46]: 0.14822770081404466 In [47]: grade ( \"tests/q1_1.py\" ) Out[47]: tests/q1_1.py: All tests passed! 1.1. Creating your own feature set Unfortunately, using all of the features has some downsides. One clear downside is computational -- computing Euclidean distances just takes a long time when we have lots of features. You might have noticed that in the last question! So we're going to select just 20. We'd like to choose features that are very discriminative . That is, features which lead us to correctly classify as much of the test set as possible. This process of choosing features that will make a classifier work well is sometimes called feature selection , or more broadly feature engineering . Question 1.1.1 Look through the list of features (the labels of the lyrics table after the first three). Choose 20 common words that you think might let you distinguish between country and hip-hop songs. Make sure to choose words that are frequent enough that every song contains at least one of them. Don't just choose the 20 most frequent, though... you can do much better. You might want to come back to this question later to improve your list, once you've seen how to evaluate your classifier. The first time you answer this question, spend some time looking through the features, but not more than 15 minutes. In [48]: most_common ( 'Artist' , train_lyrics ) Out[48]: 'Brad Paisley' In [49]: feat = Table () . with_column ( 'Features' , feature_names ) feat . sample ( 1000 ) . show ( 1000 ) Features durch u brick girlfriend komm revolut borrow tenemo someon beau glide mis chain they lost partner suprem vera blew habla yeux hold truth paus stick mc brush definit explor major worri bubbl così suppos memoria beggin work georg geh bounc comb stiff ello peur hai parc small mate doubl control tener miedo taught along cd stall por fountain paul canto ment headlight forgiven dub seventh mist attack romant lip gent south zeit ribbon crucifi anchor riot of vuelva graviti babylon soll south brake control fall defeat am escuchar attract needl deja monster mirar paradis structur jest although piel itch creo sol bug die fine been cannot cover lento wish muss joy mom monkey tiempo jamaica beso further liber much bark din i'v found querer gut cigarett mio vient stream forma prima und understood hin mentira paid organ autr viel volta shelf pant être with modo kommer gasolin advic band gain amanec warn gent 20 shade symbol aujourdhui etwa tellin abandon devour favourit fazer via posso 5 colour faint eas famili squar jsui aquello digo now devast farther letra prison killin until hopeless nois shield piec cuando cruz more dr quizá sha parc daylight robin plagu photo gritar honor lifetim sta truli recogn flaw romanc between mala fiction tank terribl about perder commit smoke kun rewind mysteri bom bli lust vez pardon wolv hollywood opportun walkin humbl kneel ist everlast poni virus convinc masquerad alibi tempt memori grain ficar heartach heart letzt cuz case thrown slippin zoo sell curl deliv overflow boat alma scheme cheat ahí gear nyt bist honest net plein shop perdida voglio last hustl cat quedo soll freund ficar author einem perman cowboy arriba kaikki depui background bone drown rule lai primera type hope view juic ingen idl daß sest mom guarda pode extrem jede sing fascin lento beef wär paus your dio themselv endless trash flesh can frame mad neither expos intens thirst melhor hammer gleich vision progress yeux sacrific neighbor 50 più routin paus saddl is ze bajo willi gain wieder bis zurück stell co dêtre flesh wenn qué wreck sword rap giorni taint niemand convinc reveng heroin ninguém dag flaw risk rout eterno cada player tragic bid absurd danc spite por some negro women afternoon almighti gun dautr plea mourn fever alegr ao cherri ohn masquerad vien wing materi secur burst highest spell cuanto dime pound empti sparkl glow glove esser howl off combat kingdom troi bright mellow ty goodnight pit loco shall des sail restless morn pocket dot modo vaya chose harmoni llora hun bet desol vein aid ir faz web concentr it pavement sabor bull print orang cada few youth mist tough phase rumor chemin vuelta deck silhouett wo virgin ruin lookin pit spi littl home uptown gana del urg insid perfect saturday scar kiss morn doigt organ civil camino either tut peu sugar da ter shiver church anna lang bliss mit dit ooh tai anyth drip collar roll conclus there herself stern shes chemic romanc ciudad jest juli jet e dag bullshit céu curl selbst wow mind ogni gentlemen garden perder sta long mexico adesso capit world cycl lo sangr spread wah no melt mira cell spite gave desir for fairi ever ku ugli fino sabor cowboy sat coração fest later begun pistol ash flaw those chest deceit thru vous princ order reel rob ana beer apolog tuesday na fuse taken están half maiden host wore q sugar fulfil g oon fou sudden thing merri hatr fou callin farm über slap comput histoir boot depoi march jen bang below carri ash tutti decis shred tall arriv sunris sorri pushin invad louder command key with date ed until miei bas remors des pig neue bello ecstasi void hospit pain made volt jean eagl robe maiden pierdo tua nein niin form mil preciso match whirl self duel brain señor nunca mann waitin besid saber pot coffe film road who basta blanco pin sail siehst junk yellow person runaway claus distancia beam empieza fun sehn du vece enjoy signal total denk bobbi siento amongst cherri from vert finest ela riot must pelo templ madrugada domani corp jumpin someday collaps maid form move forev chill photograph complet larg rain and stereo priest decay pearl bound medicin explos toca union hoje jag propheci coraz bean fiesta curs perd threat lace broke nossa sum tennesse us faut ahi achiev defens dêtre squeez wird or spiritu soleil pen men evri shinin continu glitter morgen dure puzzl freak grave disconnect world igual fui fli sólo gusta confin safeti conquer accid oooh voglio thin grant senza green von 000 terribl destin imag stack calor jo husband hypnot revers concern around bitter ow floor hay model jede flesh dead sever awaken concentr pistol ku suffoc p thrown ela forward martyr cama stress pant club expect scream it´ tooth liar faster unit lord decay tellin meant salli join earli rebel ve soi beef mama drag nuestra betti not market treat oye sea dirt cloth otro de funk magia month honey across uniform durch minut awhil immer forbidden consequ sow miracl  consum ceux ficar routin einem card jerk soil start find without market vers chair ach pleas et expect anda kickin momento sens sooth set avenu rule freeway blond sal stronger despertar forg echo retreat mondo intox author mill croi bonni cosmic weed blah scent famili focus faith ze tutto gritar pale fore heut propheci brooklyn wear 2x tree leau younger exact highway the cruz herself oli summertim ooooh redempt caer mourn mich peur woo curv ella repeat slumber ano dom misma background wind das anyway meno salir wake jealous truck harvest mamma und revolv doubt weiter shakin wa vivir skeleton tem strap bright dedic cash bailar tryin buena door wander llena cow ole star judg swallow fi complic middl past jumpin música fácil cuerpo genocid solv boca weapon vera mayb almighti ultim thug gran ihr perhap lucif himmel regret argu coup find où age 40 appreci hasta rider 1 soir compani pueblo hermano hold intox modo lick translat mucha veux vento gypsi In [50]: # Set my_20_features to an array of 20 features (strings that are column labels) my_20_features = [ 'cocain' , 'sad' , 'feel' , 'life' , 'blood' , 'fool' , 'truck' , 'jesus' , 'brother' , 'askin' , 'cheat' , 'heartbreak' , 'soul' , 'heartach' , 'hiphop' , 'angel' , 'drug' , 'thread' , 'cake' , 'mornin' ] len ( my_20_features ) train_20 = train_lyrics . select ( my_20_features ) test_20 = test_lyrics . select ( my_20_features ) This test makes sure that you have chosen words such that at least one appears in each song. If you can't find words that satisfy this test just through intuition, try writing code to print out the titles of songs that do not contain any words from your list, then look at the words they do contain. In [51]: grade ( \"tests/q1_1_1.py\" ) Out[51]: tests/q1_1_1.py: All tests passed! Next, let's classify the first song from our test set using these features. You can examine the song by running the cells below. Do you think it will be classified correctly? In [52]: print ( \"Song:\" ) test_lyrics . take ( 0 ) . select ( 'Title' , 'Artist' , 'Genre' , 'feel' ) . show () print ( \"Features:\" ) test_20 . take ( 0 ) . show () Song: Title Artist Genre feel That Kind of Love Alison Krauss Country 0.005291 Features: cocain sad feel life blood fool truck jesus brother askin cheat heartbreak soul heartach hiphop angel drug thread cake mornin 0 0 0.005291 0.005291 0 0.005291 0 0 0 0 0 0 0.010582 0.005291 0 0 0 0 0 0 As before, we want to look for the songs in the training set that are most alike our test song. We will calculate the Euclidean distances from the test song (using the 20 selected features) to all songs in the training set. You could do this with a for loop, but to make it computationally faster, we have provided a function, fast_distances , to do this for you. Read its documentation to make sure you understand what it does. (You don't need to read the code in its body unless you want to.) In [53]: # Just run this cell to define fast_distances. def fast_distances ( test_row , train_rows ): \"\"\"An array of the distances between test_row and each row in train_rows. Takes 2 arguments: test_row: A row of a table containing features of one test song (e.g., test_20.row(0)). train_rows: A table of features (for example, the whole table train_20).\"\"\" assert train_rows . num_columns < 50 , \"Make sure you're not using all the features of the lyrics table.\" counts_matrix = np . asmatrix ( train_rows . columns ) . transpose () diff = np . tile ( np . array ( test_row ), [ counts_matrix . shape [ 0 ], 1 ]) - counts_matrix distances = np . squeeze ( np . asarray ( np . sqrt ( np . square ( diff ) . sum ( 1 )))) return distances Question 1.1.2 Use the fast_distances function provided above to compute the distance from the first song in the test set to all the songs in the training set, using your set of 20 features . Make a new table called genre_and_distances with one row for each song in the training set and two columns: The \"Genre\" of the training song The \"Distance\" from the first song in the test set Ensure that genre_and_distances is sorted in increasing order by distance to the first test song . In [65]: # The staff solution took 4 lines of code. genres = train_lyrics . column ( \"Genre\" ) distances = fast_distances ( test_lyrics . select ( my_20_features ) . row ( 0 ), train_lyrics . select ( my_20_features )) genre_and_distances = Table () . with_columns ( \"Genre\" , genres , \"Distance\" , distances ) . sort ( \"Distance\" ) #dist= fast_distances(test_lyrics.select(my_20_features).row(0), train_lyrics.select(my_20_features)) #genre_and_distances = Table().with_column('Genre', genres, 'Distance',distances) #genre_and_distances.sort('Distance', descending=False) In [66]: grade ( \"tests/q1_1_2.py\" ) Out[66]: tests/q1_1_2.py: All tests passed! Question 1.1.3 Now compute the 5-nearest neighbors classification of the first song in the test set. That is, decide on its genre by finding the most common genre among its 5 nearest neighbors, according to the distances you've calculated. Then check whether your classifier chose the right genre. (Depending on the features you chose, your classifier might not get this song right, and that's okay.) In [78]: def closestk ( table , example , k ): return distances1 ( table , example ) . sort ( 'Distance' ) . take ( np . arange ( k )) def majority_genre ( neighbors ): \"\"\"Return the class that's most common among all these neighbors.\"\"\" return neighbors . group ( 'Genre' ) . sort ( 'count' , descending = True ) . column ( 'Genre' ) . item ( 0 ) neighbors = genre_and_distances . take ( np . arange ( 5 )) print ( neighbors ) # Set my_assigned_genre to the most common genre among these. my_assigned_genre = majority_genre ( neighbors ) print ( my_assigned_genre ) print ( test_lyrics . column ( 'Genre' ) . item ( 0 )) # Set my_assigned_genre_was_correct to True if my_assigned_genre # matches the actual genre of the first song in the test set. my_assigned_genre_was_correct = my_assigned_genre == test_lyrics . column ( 'Genre' ) . item ( 0 ) print ( \"The assigned genre, {} , was {} correct.\" . format ( my_assigned_genre , \" \" if my_assigned_genre_was_correct else \" not \" )) Genre | Distance Hip-hop | 0.00753402 Hip-hop | 0.00879976 Country | 0.0093184 Country | 0.00975644 Hip-hop | 0.00976657 Hip-hop Country The assigned genre, Hip-hop, was not correct. In [79]: grade ( \"tests/q1_1_3.py\" ) Out[79]: tests/q1_1_3.py: All tests passed! 1.2. A classifier function Now we can write a single function that encapsulates the whole process of classification. Question 1.2.1 Write a function called classify . It should take the following four arguments: A row of features for a song to classify (e.g., test_20.row(0) ). A table with a column for each feature (e.g., train_20 ). An array of classes that has as many items as the previous table has rows, and in the same order. k , the number of neighbors to use in classification. It should return the class a k -nearest neighbor classifier picks for the given row of features (the string 'Country' or the string 'Hip-hop' ). In [80]: def majority_genre ( neighbors ): \"\"\"Return the class that's most common among all these neighbors.\"\"\" return neighbors . group ( 'Genre' ) . sort ( 'count' , descending = True ) . column ( 'Genre' ) . item ( 0 ) def classify ( test_row , train_rows , train_classes , k ): \"\"\"Return the most common class among k nearest neigbors to test_row.\"\"\" distances = fast_distances ( test_row , train_rows ) genre_and_distances = Table () . with_columns ( \"Genre\" , genres , \"Distance\" , distances ) . sort ( \"Distance\" ) neighbors = genre_and_distances . take ( np . arange ( k )) return majority_genre ( neighbors ) classify ( test_20 . row ( 0 ), train_20 , train_lyrics . column ( \"Genre\" ), 5 ) Out[80]: 'Hip-hop' In [81]: grade ( \"tests/q1_2_1.py\" ) Out[81]: tests/q1_2_1.py: All tests passed! Question 1.2.2 Assign grandpa_genre to the genre predicted by your classifier for the song \"Grandpa Got Runned Over By A John Deere\" in the test set, using 9 neighbors and using your 20 features. In [83]: # The staff solution first defined a row object called grandpa_features. grandpa_features = my_20_features grandpa_genre = classify ( test_lyrics . where ( 'Title' , \"Grandpa Got Runned Over By A John Deere\" ) . select ( grandpa_features ) . row ( 0 ), train_20 , train_lyrics . column ( 'Genre' ), 9 ) grandpa_genre Out[83]: 'Country' In [84]: grade ( \"tests/q1_2_2.py\" ) Out[84]: tests/q1_2_2.py: All tests passed! Finally, when we evaluate our classifier, it will be useful to have a classification function that is specialized to use a fixed training set and a fixed value of k . Question 1.2.3 Create a classification function that takes as its argument a row containing your 20 features and classifies that row using the 5-nearest neighbors algorithm with train_20 as its training set. In [85]: def majority_genre ( neighbors ): \"\"\"Return the class that's most common among all these neighbors.\"\"\" return neighbors . group ( 'Genre' ) . sort ( 'count' , descending = True ) . column ( 'Genre' ) . item ( 0 ) def classify_one_argument ( row ): \"\"\"Return the most common class among k nearest neigbors to test_row.\"\"\" distances = fast_distances ( row , train_20 ) genre_and_distances = Table () . with_columns ( \"Genre\" , genres , \"Distance\" , distances ) . sort ( \"Distance\" ) neighbors = genre_and_distances . take ( np . arange ( 5 )) return majority_genre ( neighbors ) # When you're done, this should produce 'Hip-hop' or 'Country'. classify_one_argument ( test_20 . row ( 0 )) Out[85]: 'Hip-hop' In [86]: grade ( \"tests/q1_2_3.py\" ) Out[86]: tests/q1_2_3.py: All tests passed! 1.3. Evaluating your classifier Now that it's easy to use the classifier, let's see how accurate it is on the whole test set. Question 1.3.1. Use classify_one_argument and apply to classify every song in the test set. Name these guesses test_guesses . Then , compute the proportion of correct classifications. In [101]: def count_zero ( array ): \"\"\"Counts the number of 0's in an array\"\"\" return len ( array ) - np . count_nonzero ( array ) def count_equal ( array1 , array2 ): \"\"\"Takes two numerical arrays of equal length and counts the indices where the two are equal\"\"\" return count_zero ( array1 - array2 ) def evaluate_accuracy ( training , test , k ): test_attributes = test . drop ( 'Class' ) def classify_testrow ( row ): return classify ( training , row , k ) c = test_attributes . apply ( classify_testrow ) return count_equal ( c , test . column ( 'Class' )) / test . num_rows test_guesses = test_20 . apply ( classify_one_argument ) #print(test_guesses) a = test_guesses == test_lyrics . column ( 'Genre' ) num_correct = np . sum ( a ) proportion_correct = num_correct / ( train_lyrics . num_rows ) proportion_correct Out[101]: 0.2933220625528318 In [102]: grade ( \"tests/q1_3_1.py\" ) Out[102]: tests/q1_3_1.py: All tests passed! At this point, you've gone through one cycle of classifier design. Let's summarize the steps: From available data, select test and training sets. Choose an algorithm you're going to use for classification. Identify some features. Define a classifier function using your features and the training set. Evaluate its performance (the proportion of correct classifications) on the test set. Submission Congratulations! You're finished with lab 5 and Data 8.3x! You've created your own song classifer using k-nearest neighbors. If you want to continue, you can read about classification online and try to build an even more accurate classifier. In order to successfully submit your assignment, follow these steps... IMPORTANT Before you do anything, Save and Checkpoint from the File menu. Please do this first before running the cell below, run all the tests and verify that they all pass (the next cell has a shortcut for that), Review the notebook one last time, we will be grading the final state of your notebook If you make any changes, please Save and Checkpoint again. In [103]: # For your convenience, you can run this cell to run all the tests at once! import glob from okgrade.notebook import grade_notebook if not globals () . get ( '__OKGRADE__' , False ): display ( grade_notebook ( 'lab05.ipynb' , sorted ( glob . glob ( 'tests/q*.py' )))) Grade is: 100% In [ ]:","tags":"misc","url":"articles/2019/Jan/jnb-tests/"},{"title":"\"Next steps with USACE\"","text":"What remains to be done?","tags":"misc","url":"articles/2019/Jan/next-steps-with-usace/"},{"title":"\"Outline of KTC curriculum for computing and data science\"","text":"Stages of familiarity with computing and Internet Mobile device: Familiarity with carrier codes, costs; constantly updated reviews of devices, deals, costs Messaging Mail Browser M-pesa and other payment systems Video Images: photos, family pictures Laptop devices; Network Browsers Microsoft applications: Word, Excel; Other open applications: other word processors; open spreadsheets; command line interface File system structure;","tags":"misc","url":"articles/2019/Jan/outline-of-ktc-curriculum-for-computing-and-data-science/"},{"title":"\"Tool Survey\"","text":"Tool Survey Four major tools: 1. Atom editor; ties into github; manages file structure on mac; uses Markdown syntax; provides shortcuts for Markdown; simple table editing (though not clear how easy to export HTML or cut and paste table into other formats such as Numbers, or TextEdit); does have parallel visualization of final rendered version, reached by Control&#94;M; 2. Github: use for file control, to maintain updated version on local machine, and then synced version on cloud at Github; two rival git systems: a, b; 1. Use github for web publishing, by placing files in a special place, which is then reached by a github URL. 2. Need to do daily practice with maintaining files, blog entries; 3. Need to set up organized hierarchy of blog entries; 3. Pelican: Web publishing theme: Python based; uses library of templates and add-ons; 1. Need to clean up the .py file that loads plug-ins; getting error messages; 2. Need to only load needed templates, needed plug-ins; 3. Need to explore the use made of sidebar by Portland site; 4. Need to explore integration of Jupyter notebooks; 4. Jupyter notebooks 1. How to invoke locally 2. How to invoke on Google server 3. How to invoke on other servers","tags":"misc","url":"articles/2019/Jan/tool-survey/"},{"title":"\"2019 Jan 2 Tahoe USACOE\"","text":"2 Jan 2019 Army Corps of Engineers Applicatation Status Monday, Bob Marston and I reviewed all imagery and other documentation so far assembled. We agree that we must succeed in this application, and that, so far, we have a suggestive, but not conclusive case to present to Aaron Park at USACE. I have superimposed all imagery in a GoogleEarth Pro document which allows us to compare the five major image collections. It draws on the Dropbox collection of thirty images. I will place the KML file in the Dropbox folder so Gary and Melissa can see what Bob and I reviewed. Year Visible Image: Held Image: To Acquire Comments 1939 May pier, May house, no Miller pier CDJ 14-55, 14-76 no buoys 1950 Miller pier, May pier, May buoy PAI 157 After much work, only two buoys visible; 6 small white wave dots near Miller pier 1950 Miller pier, May pier, May buoy PAI 158 Does not show same white wave dots => Miller pier dots are artifacts 1950 Same flight 149, 148, 154, 155, 156, 159 see reference doc showing all PAI overflights from FrameFinder; 157 and 158 do not show Miller buoys, so these won't, either 1952 ABM 3K-10 no buoys, some dots 1952 3K-11 will order, hope for good dots 1953 165, 166 some dots 1955 Topo shows Miller house, Miller pier, May pier, May house 1956 10-18, 10-19 lo-res 1961-1962 10-126 some dots 1961-1962 11-48 some different dots 1961-1962 10-179, 10-180, 10-181 ordered 10-180, 10-181, 10-127 from UCSB 1963 Cal DOT ASC 600-149, CAS Roll 1, expossures 138,139 query in to Radman Aerial Surveys, Sacramento, recommended by Young Lee, Chief of Photogrammetry, DOT 1968-1969 Miller pier, May pier, May buoy, Tahoe Park buoy USGS from GoogleEarth good dots; two best dots show May buoy, boat moored to buoy in Tahoe Park buoy field; other dots look almost as good as May buoy, Tahoe Park buoy with boat 1970 Tahoe shoreline: 1:300 query in to US Forestry in Salt Lake City; shut for Trump shutdown 1971 CAS 10-024","tags":"misc","url":"articles/2019/Jan/2019-jan-2-tahoe-usacoe/"},{"title":"\"Saturday Dec 29 Work Outline\"","text":"Getting up to speed to finish Tahoe Project Build complete set of imagery for talk with Bob Marston Outline presentation to USACOE List all future possible images Check with Radman Aerial Services, 6220 24 th Street, Sacramento; 916 391 1651 Check with Cal DWR:","tags":"misc","url":"articles/2018/Dec/saturday-dec-29-work-outline/"},{"title":"\"Results of visit to California Department of Transportation Photogrammetry\"","text":"met with Lee found visual index of 1963 imagery Need to contact Imagery vendor for costs of copying","tags":"misc","url":"articles/2018/Dec/results-of-visit-to-california-department-of-transportation-photogrammetry/"},{"title":"Lake Tahoe Presentation","text":"How to get the Atom template to work?","tags":"Tahoe","url":"articles/2018/Dec/lake-tahoe-presentation/"},{"title":"Pre-1972 imagery of Sunnyside, Lake Tahoe","text":"Sunnyside, Lake Tahoe: Historical Sources To build an accurate picture of the piers and buoys in Lake Tahoe over the past century, we must combine source documents from many repositories: - written documents from official sources, including Grant Deeds from Placer County, Tahoe Regional Park Authority, California State Lands Commission, and the United States Army Corps of Engineers. - Aerial and satellite imagery from a variety of sources. Section 2 contains a listing of sources and holdings. -There will be a Stakeholder Science Committee meeting on January 8, 2019, time TBD, at the Tahoe Regional Planning Agency (128 Market St, Stateline, NV 89410). - A list of source materials and their repositories, including: Source Data | | --|---|-- US Geological Survey | | US Forestry Service | | USArmy COE | | US Bureau of Land Management | | US Dep of Agriculture| | US Forest Service | | US Lake Tahoe Basin Management Unit LTBMU | 1973 | Lake Tahoe West Restoration Partnership | | | | California Dep of Transportion | | California Dep of Water Resources | | California Dep of Parks | | | | Placer County records | | Tahoe Regional Planning Association | | University of California, Davis | | University of California, Santa Barbara | | University of California, Berkeley | | University of California, Riverside| | Stanford University | | University of Nevada, Reno| | Commercial repositories | | Radman Aerial Services| | | -","tags":"Tahoe","url":"articles/2018/Dec/pre-1972-imagery-of-sunnyside-lake-tahoe/"},{"title":"C2M  Cleantech to Market","text":"C2M: Cleantech to Market Brian Steele: Innovation Council: Randy Katz","tags":"Energy","url":"articles/2018/Nov/c2m-cleantech-to-market/"},{"title":"Data Science Courses","text":"Eric...; Alex ; E-157 Environmental Justice ER 190; Duncan Calloway ER 190 HW 5 Air quality 2.5 micron K-nearest neighbors scikit-learn MCB 32: Intro to Human Physiology: Alex 421 lab students * targets: Bio 1A E 157 : Khalid Khadir SOC 130AC: social inequality: Tract data Legalst-123: Data, Prediction and Law election data into pandas ESPM-163AC: Environmetal Justice XRHETOR-R1A: The Craft of Writing Global 150: Spring: 1as 150 E 157 AC ESPM 156 CP 101 EEP 147 ds-curriculum@berkeley.edu ds-modules.GITHUB_PAGES_BRANCH Michael Cohen mcohen@newsunroad.com","tags":"Water, Data Science","url":"articles/2018/Nov/data-science-courses/"},{"title":"Berkeley Vision 2050 Commission","text":"Planning Berkeley Future Last year, following the initiative by Gordon Wozniak and John Gage, Mayor Jesse Arreguín created the Berkeley 2050 Commission, ratified this November in Berkeley by the passage of Prop R. We have four committees: Technology, Quality of Life, Financial and Management, and Environment. We plan to spend 2019 in a series of exploratory meetings with all elements of the Berkeley community, culminating in a comprehensive plan for infrastructure investment. UrbanSim would be an ideal platform, it seems, for exploration of a range of possible futures in these meetings. Berkeley will spend over $1 billion dollars in infrastructure maintenance, modification, and renovation in the next thirty years. We plan to explore the impact of new technologies on the costs of street surfacing and storm water treatment, waste water and clean water decentralization, energy system decentralization, re-engineering of all continuity systems--communications, energy distribution, water flows, transportation and materiel movement, piping systems,--and examine the costs and benefits of re-engineering all structures in Berkeley for resilience and efficiency. Michael Reilly, Metropolitan Transportation Commission, San Francisco, California Agile urban modelling to support regional planning in the San Francisco bay area Bryan T. Adey, Claudio Martani, Natalia Papathanasiou, Marcel Burkhalter, ETH Zürich Principles for estimating and communicating the sustainability of intervention programmes on infrastructure Samuel Maurer, Paul Waddell, University of California, Berkeley Template-based task orchestration for urban modelling Michael Ndwiga, Richard Mulwa, Juha Siikamaki and Jessica Alvsilver, University of Nairobi Valuing Urban Green Spaces and Nature Restoration in a Developing Country: Application of Choice Experiment Joseph Ferreira, MIT Mi Diao, National University of Singapore Diem Le, Xiaohu Zhang, Singapore/MIT Alliance for Research and Technology Roberto Ponce Lopez, Jingsi Shaw, MIT Yi Zhu, Shanghai University of Finance and Economics P. Christopher Zegras, MIT Integrating Accessibility Measures into the Microsimulation of Daily Real Estate Market Dynamics Paul Waddell, Samuel Maurer, Geoff Boeing, University of California, Berkeley Ignacio Garcia-Dorado, Google Research Max Gardner, Emily Porter, University of California, Berkeley Daniel Aliaga, Purdue University Architecture for modular microsimulation of real estate markets and transportation Michael Wegener, Spiekermann & Wegener Urban and Regional Research (S&W) Modelling Long-term Urban Change Keith C. Clarke, University of California, Santa Barbara Urbanization and global change in California Roger White, Lien Poelmans, Karolien Vermeiren, Inge Uljee, Guy Engelen, Flemish Institute for Technological Research Long term simulation of urban development scenarios in support of spatial policy planning https://www.nytimes.com/interactive/2016/05/18/upshot/which-buildings-in-manhattan-couldnt-be-built-again-today.html http://www.quantierra.com/","tags":"Cities","url":"articles/2018/Nov/berkeley-vision-2050-commission/"},{"title":"Digital Plant Operational Data","text":"Notes on Rachel Slabaugh Nuclear Plant Digital Data and Machine Learning BIDS presentation Tuesday, 27 Nov 2018 Nuclear plants are 3-5X more expensive to build and maintain than fossil fuel Major operational and maintenance costs are personnel Example of Southern Nuclear , one of US nuclear operators: 6 months of data is 1 Terabyte: reveals how little is monitored. No retention of older data * Little analysis, not set up to allow machine learning Need physics-based models with machine learning parameter estimation Battery Fault Detection: capture beginning of temperature rise; do not wait until threshold temperature is reached, with damage already present. Use Kalman filter with Theta = 0.3 to catch beginning","tags":"Water","url":"articles/2018/Nov/digital-plant-operational-data/"},{"title":"Lake Tahoe Imagery","text":"[[toc]] [ ] Do This [x] words Testing links :sunrise: a link to works by John Gage Go to Google a link to works by Paul Krugman See the hyphen What does this do? [#hyph] a link to Batteries relative to the current file [Where to go][] link to index link to headline link to buoy link to tail end tag Tahoe Imagery Area of Interest Corner Lat Long NW 39.143860 -120.152582 NE 39.143912 -120.150970 SE 39.141406 -120.150828 SW 39.141674 -120.152616 this is underscore Sites Links USGS https://pubs.usgs.gov/ds/376/ this is [hyphen]: #hyph A List of imagery Date Description Location Series Call Number UC Davis aerial photo library 1940 HDOs for the southern Lake Tahoe Basin . . 1969 HDOs for the entire Lake Tahoe Basin . . 1962 Photo mosaic index sheet 1-14 Project covers complete county No Photos PLA G4363.P5A4 1962. C2 None G4363.P5A4 1971. C3 1962 Photo mosaic index sheet 1-14 Project covers complete county No Photos PLA G4363.P5A4 1962. C2 None 1964 Photo mosaic indexsheet 1-4 Project covers western portion of county No Photos ABM G4363.P5A4 1964. U6 None 1971 Photo mosaic index sheet 1-7 Project covers complete county Complete photoset of project area 2492 [errror: 2942] G4363.P5A4 1971. C3 Non-stereo 1971 Photo mosaic index sheet 1-2 Project covers western portion of county No Photos ABM G4363.P5A4 1971. U6 None 1938 Photo mosaic index sheet 1-3 Project covers western portion of county Complete photoset of project area ABM G4363.P5A4 1938. U6 Non-stereo Photoset includes large 20x25in. aerial photographs 1969 Richerson, Peter J. Community ecology of the Lake Tahoe plankton. [Davis, Calif.] 1969. UCD Special Collections (NRLF) LD781 D5J 1969 R535 AVAILABLE RESTRICTED USE Description 111 \\U+25a1. illus. Thesis Thesis--University of California, Davis. UCSB listing of all imagery for Placer County http://mil.library.ucsb.edu/apcatalog/ap_indexes/county.php?county_id=215&state_id=5 Framefinder monody-cagey-fiesta-untrue 39.1422605,-120.1521439 Buoy Lat Long Lovewell 39.142418 -120.151224 May 39.142275 -120.151335 Gage 39.142242 -120.151583 Marston 39.142099 -120.151378 VDB1 39.141968 -120.151730 VDB2 39.141872 -120.151867 VDB3 39.141862 -120.152068 VDB4 39.141700 -120.152170 rmarston@telstarinc.com, Melisssa@midkiffandassoc.com, gary@midkiffandassoc.com, gagemail@comcast.net http://dev.virtualearth.net/REST/v1/Imagery/Metadata/Aerial/55.923,-133.653?zl=19&key=AoDUA8UQYQE4MBLf4G6XJqmmrbXZG9wdhFag0jbbcF2RNZwnNQnP4_tJ-FYL45GP Viewing the ERDAS_IMG multi-layered images requires specialized software. A free desktop viewer, ERDAS ER Viewer is available from Hexagon Geospatial. ERDAS_IMG files can be viewed in GeoViewer, freely downloadable from LizardTech. ERDAS_IMG files can be read and written by ArcGIS Desktop and other ESRI applications. See Supported raster dataset file formats from ArcGIS help. It is also supported by Safe Software's FME engine for format conversion. There appear to be two software libraries that support the reading and writing of ERDAS_IMG files: The open source GDAL/OGR library. See GDAL: HFA -- Erdas Imagine .img. This capability in GDAL derives from a project evidently supported by Intergraph around 2004. Intergraph was owner of ERDAS IMAGINE software at the time. This project yielded a C++ API that is integrated into GDAL and also a utility img2tif designed to convert ERDAS_IMG files to GeoTIFFs. As of January 2015, the former link to the source code for img2tif (with a date of 2006) is no longer online. See former project page via the Internet Archive for more information. The compilers of this resource do not know whether GDAL support handles new variants of the ERDAS_IMG format. Comments welcome. This library is used by the open source QGIS and GRASS applications. The IMAGINE Developers' Toolkit, included in the Producer suite of the Power Portfolio from Hexagon Geospatial. This toolkit supports reading and writing of ERDAS_IMG files. Tail end tag Section 1 The formula, \\(y=mx+c\\) , is displayed inline. Some symbols and equations (such as \\(\\sum{x}\\) or \\(\\frac{1}{2}\\) ) are rescaled to prevent disruptions to the regular line spacing. For more voluminous equations (such as \\(\\sum{\\frac{(\\mu - \\bar{x})&#94;2}{n-1}}\\) ), some line spacing disruptions are unavoidable. Math should then be displayed in displayed mode. $$\\\\sum{\\frac{(\\mu - \\bar{x})&#94;2}{n-1}}$$ if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Tahoe","url":"articles/2018/Nov/lake-tahoe-imagery/"},{"title":"Elections--United States: 2018","text":"This will be turned into HTML . Structural Problems Ballot Design Absentee Counting Process How We Vote Diagram of all Election Elements","tags":"Power","url":"articles/2018/Nov/Voting/"},{"title":"A rst document","text":"This is it... and more...... Why won't restructured text appear?","tags":"Tools","url":"articles/2018/Nov/my-super-post/"},{"title":"littletable Done in HTML","text":"Header level 2 Is there a way to set the date? Caption Now add what you want to see numerically 1 What new words? aaaa aaaa aaaa aaaa this is 4 2 cccc; aaaa aaaa aaaa http://google.com 3 aaaa aaaa aaaa Water 4 aaaa aaaa aaaa #BBD0E0","tags":"Tools","url":"articles/2018/Nov/littletable-done-in-html/"},{"title":"How This Site Was Created","text":"This site was patterned on the site created by Peter Kazarinoff, Professor of Engineering at Portland Community College He describes the tools, the editors, and the templates needed for a static web site. His site is He has intriguing tools for conversion of Jupyter notebooks into LaTex. Using his list of Pelican plugins. Using a modification of his pelicanconf.py files. Some problems with final rendering, and with publishconf.py November 13, 2018 The author of Pelican, Alexis Metaireau, and the current maintainer of the pelican github site, Justin Meyer and 35 others, just released Pelican 4.0. My site could not build new html pages, and gave the error: ERROR : Cannot load plugin `i18n_subsites` | ImportError : cannot import name 'Draft' / Users / johngage / Documents / blog / pelican - plugins / liquid_tags / notebook . py : 70 : UserWarning : Pelican plugin is not designed to work with IPython versions greater than 1. x . CSS styles have changed in later releases . warnings . warn ( \"Pelican plugin is not designed to work with IPython \" ** Writing styles to _nb_header . html : this should be included in the theme . ** CRITICAL : UndefinedError : 'gettext' is undefined I added two lines to the basic Python for Pelican in /Users/johngage/anaconda3/lib/python3.6/site-packages/pelican/generators.py On line 28, just after from pelican.utils import ( DateFormatter , copy , mkdir_p , order_content , posixize_path , process_translations , python_2_unicode_compatible ) add (on line 27) import gettext and in the same file, on line 78, just after self.env = Environment( loader=ChoiceLoader([ FileSystemLoader(self._templates_path), simple_loader, # implicit inheritance PrefixLoader({ '!simple': simple_loader, '!theme': theme_loader }) # explicit ones ]), **self.settings['JINJA_ENVIRONMENT'] ) add: #insert fix for gettext error,using Baky's Blog code Nov 2018 self.env.install_gettext_translations( gettext ) This seemed to get rid of the gettext undefined error, and the site rendered.","tags":"Tools","url":"articles/2018/Nov/how-this-site-was-created/"},{"title":"UC Berkeley Data Science 8.1x Lab 5","text":"Incorporating Jupyter notebooks This post contains a Jupyter notebook from the 2018 edX series on Data Science from UC Berkley. The series included three segments, and will be repeated in January, 2019. Here is the link to the version stored on UCBerkeley hub: Lab 5: Gapminder animation of World Bank and WHO data This is the inclusion of the actual Jupyter notebook; pelican converts it to pure HTML, using a long CSS file, and stores the HTML in the \"notebook\" subdirectory of \"content\". In [1]: 6 + 9 Out[1]: 15 Lab 5: World Progress Welcome to lab 5! This final lab in Data 8.1x brings together many of the topics so far, including data table manipulation, visualization, and iteration. The content of the lab is based on a series of talks by Hans Rosling, a statistician who advised many world leaders about the changing state of the world's population. (Optional) For a video introduction to the topic of Global population change, you can watch Hans Rosling's video, Don't Panic: The Facts About Population . First, set up the tests and imports by running the cell below. In [2]: # Run this cell to set up the notebook, but please don't change it. # These lines import the Numpy and Datascience modules. import numpy as np from datascience import * # These lines do some fancy plotting magic. import matplotlib % matplotlib inline import matplotlib.pyplot as plots plots . style . use ( 'fivethirtyeight' ) from ipywidgets import interact , interactive , fixed , interact_manual import ipywidgets as widgets from gofer.ok import check The global population of humans reached 1 billion around 1800, 3 billion around 1960, and 7 billion around 2011. The potential impact of exponential population growth has concerned scientists, economists, and politicians alike. The UN Population Division estimates that the world population will likely continue to grow throughout the 21st century, but at a slower rate, perhaps reaching 11 billion by 2100. However, the UN does not rule out scenarios of more extreme growth. In this section, we will examine some of the factors that influence population growth and how they are changing around the world. The first table we will consider is the total population of each country over time. Run the cell below. In [29]: # The population.csv file can also be found online here: # https://github.com/open-numbers/ddf--gapminder--systema_globalis/raw/master/ddf--datapoints--population_total--by--geo--time.csv # The version in this project was downloaded in February, 2017. population = Table . read_table ( 'population.csv' ) population . show ( 40 ) geo time population_total abw 1800 19286 abw 1801 19286 abw 1802 19286 abw 1803 19286 abw 1804 19286 abw 1805 19286 abw 1806 19286 abw 1807 19286 abw 1808 19286 abw 1809 19286 abw 1810 19286 abw 1811 19286 abw 1812 19286 abw 1813 19286 abw 1814 19286 abw 1815 19286 abw 1816 19286 abw 1817 19286 abw 1818 19286 abw 1819 19286 abw 1820 19286 abw 1821 19387 abw 1822 19489 abw 1823 19591 abw 1824 19694 abw 1825 19797 abw 1826 19901 abw 1827 20005 abw 1828 20110 abw 1829 20216 abw 1830 20322 abw 1831 20428 abw 1832 20535 abw 1833 20643 abw 1834 20751 abw 1835 20860 abw 1836 20969 abw 1837 21079 abw 1838 21190 abw 1839 21301 ... (87755 rows omitted) 1. Bangladesh In the population table, the geo column contains three-letter codes established by the International Organization for Standardization (ISO) in the Alpha-3 standard. We will begin by taking a close look at Bangladesh. Inspect the standard to find the 3-letter code for Bangladesh. Question 1.1 Create a table called b_pop that has two columns labeled time and population_total . The first column should contain the years from 1970 through 2015 (including both 1970 and 2015) and the second should contain the population of Bangladesh in each of those years. In [30]: b_pop = population . select ( 'geo' , 'time' , 'population_total' ) . where ( 'geo' , are . equal_to ( \"bgd\" )) . where ( 'time' , are . between_or_equal_to ( 1970 , 2015 )) b_pop = b_pop . drop ( 'geo' ) b_pop Out[30]: time population_total 1970 65048701 1971 66417450 1972 67578486 1973 68658472 1974 69837960 1975 71247153 1976 72930206 1977 74848466 1978 76948378 1979 79141947 ... (36 rows omitted) In [ ]: check ( 'tests/q1_1.py' ) Run the following cell to create a table called b_five that has the population of Bangladesh every five years. At a glance, it appears that the population of Bangladesh has been growing quickly indeed! In [ ]: b_pop . set_format ( 'population_total' , NumberFormatter ) fives = np . arange ( 1970 , 2016 , 5 ) # 1970, 1975, 1980, ... threes = np . arange ( 1970 , 2016 , 3 ) b_five = b_pop . sort ( 'time' ) . where ( 'time' , are . contained_in ( fives )) b_three = b_pop . sort ( 'time' ) . where ( 'time' , are . contained_in ( threes )) print ( b_three ) print ( b_five ) b_three . bar ( 'time' ) #print(type(b_five)) Run the next cell to create a table called b_five_growth which shows the growth rate for each five-year period from 1970 through 2010. In [ ]: b_1970_through_2010 = b_five . where ( 'time' , are . below_or_equal_to ( 2010 )) b_five_growth = b_1970_through_2010 . with_column ( 'annual_growth' , ( b_five . exclude ( 0 ) . column ( 1 ) / b_1970_through_2010 . column ( 1 )) ** 0.2 - 1 ) b_five_growth . set_format ( 'annual_growth' , PercentFormatter ) In [ ]: print ( b_five . num_rows ) #this is why we don't go to 2015 print ( b_1970_through_2010 . num_rows ) b_five_growth . num_rows While the population has grown every five years since 1970, the annual growth rate decreased dramatically from 1985 to 2005. Let's look at some other information in order to develop a possible explanation. Run the next cell to load three additional tables of measurements about countries over time. In [31]: life_expectancy = Table . read_table ( 'life_expectancy.csv' ) child_mortality = Table . read_table ( 'child_mortality.csv' ) . relabeled ( 2 , 'child_mortality_under_5_per_1000_born' ) fertility = Table . read_table ( 'fertility.csv' ) print ( fertility ) print ( child_mortality ) geo | time | children_per_woman_total_fertility afg | 1800 | 7 afg | 1801 | 7 afg | 1802 | 7 afg | 1803 | 7 afg | 1804 | 7 afg | 1805 | 7 afg | 1806 | 7 afg | 1807 | 7 afg | 1808 | 7 afg | 1809 | 7 ... (43402 rows omitted) geo | time | child_mortality_under_5_per_1000_born afg | 1800 | 468.6 afg | 1801 | 468.6 afg | 1802 | 468.6 afg | 1803 | 468.6 afg | 1804 | 468.6 afg | 1805 | 468.6 afg | 1806 | 470 afg | 1807 | 470 afg | 1808 | 470 afg | 1809 | 470 ... (40746 rows omitted) The life_expectancy table contains a statistic that is often used to measure how long people live, called life expectancy at birth . This number, for a country in a given year, does not measure how long babies born in that year are expected to live . Instead, it measures how long someone would live, on average, if the mortality conditions in that year persisted throughout their lifetime. These \"mortality conditions\" describe what fraction of people at each age survived the year. So, it is a way of measuring the proportion of people that are staying alive, aggregated over different age groups in the population. The fertility table contains a statistic that is often used to measure how many babies are being born, the total fertility rate . This number describes the number of children a woman would have in her lifetime , on average, if the current rates of birth by age of the mother persisted throughout her child bearing years, assuming she survived through age 49. Question 1.2. Write a function fertility_over_time that takes the Alpha-3 code of a country and a start year. It returns a two-column table with labels \" Year \" and \" Children per woman \" that can be used to generate a line chart of the country's fertility rate each year, starting at the start year. The plot should include the start year and all later years that appear in the fertility table. Then, in the next cell, call your fertility_over_time function on the Alpha-3 code for Bangladesh and the year 1970 in order to plot how Bangladesh's fertility rate has changed since 1970. Note that the function fertility_over_time should not return the plot itself The expression that draws the line plot is provided for you; please don't change it. In [32]: def fertility_over_time ( country , start ): \"\"\"Create a two-column table that describes a country's total fertility rate each year.\"\"\" country_fertility = fertility . select ( 'geo' , 'time' , 'children_per_woman_total_fertility' ) . where ( 'geo' , are . containing ( country )) . where ( 'time' , are . above_or_equal_to ( start )) country_fertility_after_start = country_fertility . relabeled ( 'time' , 'Year' ) . relabeled ( 'children_per_woman_total_fertility' , 'Children per woman' ) . drop ( 'geo' ) return country_fertility_after_start In [33]: bangladesh_code = 'bgd' fertility_over_time ( bangladesh_code , 1970 ) . plot ( 0 , 1 ) # You should *not* change this line. In [34]: check ( 'tests/q1_2.py' ) /home/jovyan/materials-x18/materials/x18/lab/1/lab05/tests/q1_2.py: All tests passed! Question 1.3. Using both the fertility and child_mortality tables, draw a scatter diagram with one point for each year, starting with 1970, that has Bangladesh's total fertility on the horizontal axis and its child mortality on the vertical axis. The expression that draws the scatter diagram is provided for you; please don't change it. Instead, create a table called post_1969_fertility_and_child_mortality with the appropriate column labels and data in order to generate the chart correctly. Use the label \" Children per woman \" to describe total fertility and the label \" Child deaths per 1000 born \" to describe child mortality. In [35]: bgd_fertility = fertility . select ( 'geo' , 'time' , 'children_per_woman_total_fertility' ) . where ( 'geo' , are . containing ( 'bgd' )) . drop ( 'geo' ) bgd_child_mortality = child_mortality . select ( 'geo' , 'time' , 'child_mortality_under_5_per_1000_born' ) . where ( 'geo' , are . containing ( 'bgd' )) . drop ( 'geo' ) fertility_and_child_mortality = bgd_fertility . join ( 'time' , bgd_child_mortality , 'time' ) . relabeled ( 'children_per_woman_total_fertility' , 'Children per woman' ) . relabeled ( 'child_mortality_under_5_per_1000_born' , 'Child deaths per 1000 born' ) print ( fertility_and_child_mortality ) post_1969_fertility_and_child_mortality = fertility_and_child_mortality . select ( 'time' , 'Children per woman' , 'Child deaths per 1000 born' ) . where ( 'time' , are . above_or_equal_to ( 1970 )) post_1969_fertility_and_child_mortality . scatter ( 'Children per woman' , 'Child deaths per 1000 born' ) # You should *not* change this line. time | Children per woman | Child deaths per 1000 born 1800 | 6.7 | 507.9 1801 | 6.7 | 507.9 1802 | 6.7 | 507.9 1803 | 6.7 | 507.9 1804 | 6.7 | 507.9 1805 | 6.7 | 507.9 1806 | 6.7 | 507.9 1807 | 6.7 | 507.9 1808 | 6.7 | 507.9 1809 | 6.7 | 507.9 ... (206 rows omitted) In [ ]: check ( 'tests/q1_3.py' ) 2. The World The change observed in Bangladesh since 1970 can also be observed in many other developing countries: health services improve, life expectancy increases, and child mortality decreases. At the same time, the fertility rate often plummets, and so the population growth rate decreases despite increasing longevity. Run the next cell to see a line plot of the world population from 1800 through 2005. You might recognize some of the code used! In [36]: population . where ( 'time' , are . between ( 1800 , 2006 )) . drop ( 'geo' ) . group ( 'time' , sum ) . plot ( 0 ) Question 2.1. Create a function stats_for_year that takes a year and returns a table of statistics. The table it returns should have four columns: geo , population_total , children_per_woman_total_fertility , and child_mortality_under_5_per_1000_born . Each row should contain one Alpha-3 country code and three statistics: population, fertility rate, and child mortality for that year from the population , fertility and child_mortality tables. Only include rows for which all three statistics are available for the country and year. In addition, restrict the result to country codes that appears in big_50 , an array of the 50 most populous countries in 2010. This restriction will speed up computations later in the project. Hint : The tests for this question are quite comprehensive, so if you pass the tests, your function is probably correct. However, without calling your function yourself and looking at the output, it will be very difficult to understand any problems you have, so try your best to write the function correctly and check that it works before you rely on the ok tests to confirm your work. In [37]: # We first create a population table that only includes the # 50 countries with the largest 2010 populations. We focus on # these 50 countries only so that plotting later will run faster. #print(population.group('geo').num_rows) #print(fertility.group('geo').num_rows) #print(child_mortality.group('geo').num_rows) big_50 = population . where ( 'time' , 2010 ) . sort ( 2 , descending = True ) . take ( np . arange ( 50 )) . column ( 'geo' ) #array of geocodes population_of_big_50 = population . where ( 'time' , are . above ( 1959 )) . where ( 'geo' , are . contained_in ( big_50 )) # Want to test for all three values non-zero; want to extract from three tables; want to use for loop def stats_for_year ( year ): \"\"\"Return a table of the stats for each country that year.\"\"\" p = population . where ( 'time' , year ) . drop ( 'time' ) #.where('geo', are.contained_in(big_50)) f = fertility . where ( 'time' , year ) . drop ( 'time' ) #.where('geo', are.contained_in(big_50)) c = child_mortality . where ( 'time' , year ) . drop ( 'time' ) #.where('geo',are.contained_in(big_50)) yearstat = p . join ( 'geo' , f , \"geo\" ) #pop joins with fertility yearstat = yearstat . join ( 'geo' , c , 'geo' ) #pop+fertility joins with child mortality;only includes non-zero rows yearstat = yearstat . where ( 'geo' , are . contained_in ( big_50 )) #pop:255;fertility:201;child:211: alltogether in yearstat:183 return yearstat Try calling your function stats_for_year on any year between 1960 and 2010 in the cell below. Try to understand the output of stats_for_year . In [38]: stats_for_year ( 1960 ) . show ( 500 ) geo population_total children_per_woman_total_fertility child_mortality_under_5_per_1000_born afg 8994793 7.67 362.4 arg 20619075 3.11 73.2 bgd 48200702 6.73 264.3 bra 72493585 6.21 171.8 can 17909232 3.91 32.6 chn 644450173 3.99 309 cod 15248246 6 266 col 16480384 6.81 127.3 deu 73179665 2.41 42.87 dza 11124892 7.65 245.7 egy 27072397 6.63 312.8 esp 30450994 2.77 55.6 eth 22151218 6.88 286.4 fra 45865699 2.77 28.5 gbr 52410496 2.69 26.6 gha 6652285 6.75 210.9 idn 87792512 5.67 222.4 ind 449661874 5.87 247.7 irn 21906905 6.93 300.1 irq 7289759 6.25 195.7 ita 49714962 2.37 52 jpn 92500754 2 39.7 ken 8105440 7.95 198.6 kor 25074028 6.16 113 mar 12328534 7.07 239.4 mex 38174114 6.78 142.9 mmr 21486424 6.05 276.7 moz 7493278 6.6 300.8 mys 8160975 6.19 92.8 nga 45211614 6.35 337.3 npl 10056945 5.99 327.1 pak 44911810 6.6 261.9 per 10061519 6.88 227.4 phl 26273023 7.15 106 pol 29716363 2.97 64.7 prk 11424179 4.58 127.3 rus 119860289 2.56 55.81 sau 4086539 7.22 241.9 sdn 7527450 6.69 178.4 tha 27397178 6.15 147.9 tur 27553280 6.3 249 tza 10074490 6.81 243.4 uga 6788211 7 223.4 ukr 42662150 2.42 52.66 usa 186176524 3.67 30.1 uzb 8789492 6.71 175.7 ven 8146845 6.62 81.1 vnm 32670623 6.35 110.1 yem 5166311 7.29 425.5 zaf 17396367 6.17 194.9 In [39]: check ( 'tests/q2_1.py' ) /home/jovyan/materials-x18/materials/x18/lab/1/lab05/tests/q2_1.py: All tests passed! Question 2.2. Create a table called pop_by_decade with two columns called decade and population . It has a row for each year since 1960 that starts a decade. The population column contains the total population of all countries included in the result of stats_for_year(year) for the first year of the decade. For example, 1960 is the first year of the 1960's decade. You should see that these countries contain most of the world's population. Hint: It may be helpful to use the provided pop_for_year that computes this total population, then apply it to the decade column. In [40]: def pop_for_year ( year ): return sum ( stats_for_year ( year ) . column ( 'population_total' )) def worldpop ( year ): worldpop = population . select ( 'geo' , 'population_total' , 'time' ) . where ( 'time' , are . equal_to ( year )) #.where(geo', are.contained_in(big_50)) return sum ( worldpop . column ( 'population_total' )) print ( 'Pop of big 50 for 1960' , pop_for_year ( 1960 )) print ( 'The proportion of world population of the big 50 in 1960 is' , pop_for_year ( 1960 ) / worldpop ( 1960 )) print ( 'The proportion of world population of the big 50 in 2015 is' , pop_for_year ( 2015 ) / worldpop ( 2015 )) Pop of big 50 for 1960 2624944597 The proportion of world population of the big 50 in 1960 is 0.7675191988278263 The proportion of world population of the big 50 in 2015 is 0.8692894892060716 In [41]: decades = Table () . with_column ( 'decade' , np . arange ( 1960 , 2011 , 10 )) #decades.apply(pop_for_year,'decade') pop_by_decade = decades . with_column ( 'population' , decades . apply ( pop_for_year , 'decade' ) ) pop_by_decade . set_format ( 1 , NumberFormatter ) Out[41]: decade population 1960 2,624,944,597 1970 3,211,487,418 1980 3,880,722,003 1990 4,648,434,558 2000 5,367,553,063 2010 6,040,810,517 In [42]: check ( 'tests/q2_2.py' ) /home/jovyan/materials-x18/materials/x18/lab/1/lab05/tests/q2_2.py: All tests passed! The countries table describes various characteristics of countries. The country column contains the same codes as the geo column in each of the other data tables ( population , fertility , and child_mortality ). The world_6region column classifies each country into a region of the world. Run the cell below to inspect the data. In [43]: countries = Table . read_table ( 'countries.csv' ) . where ( 'country' , are . contained_in ( population . group ( 'geo' ) . column ( 0 ))) countries . select ( 'country' , 'name' , 'world_6region' ) Out[43]: country name world_6region afg Afghanistan south_asia akr_a_dhe Akrotiri and Dhekelia europe_central_asia alb Albania europe_central_asia dza Algeria middle_east_north_africa asm American Samoa east_asia_pacific and Andorra europe_central_asia ago Angola sub_saharan_africa aia Anguilla america atg Antigua and Barbuda america arg Argentina america ... (245 rows omitted) Question 2.3. Create a table called region_counts that has two columns, region and count . It should describe the count of how many countries in each region appear in the result of stats_for_year(1960) . For example, one row would have south_asia as its world_6region value and an integer as its count value: the number of large South Asian countries for which we have population, fertility, and child mortality numbers from 1960. In [44]: #print(countries.num_rows) #print(stats_for_year(1960).num_rows) #print(population.group('geo').num_rows) #print(fertility.group('geo').num_rows) #print(child_mortality.group('geo').num_rows) region_counts = stats_for_year ( 1960 ) . where ( 'geo' , are . contained_in ( big_50 )) . join ( 'geo' , countries , 'country' ) . relabeled ( 'world_6region' , 'region' ) . group ( 'region' ) print ( region_counts ) region_counts . labels region | count america | 8 east_asia_pacific | 10 europe_central_asia | 10 middle_east_north_africa | 7 south_asia | 5 sub_saharan_africa | 10 Out[44]: ('region', 'count') In [45]: check ( 'tests/q2_3.py' ) /home/jovyan/materials-x18/materials/x18/lab/1/lab05/tests/q2_3.py: All tests passed! The following scatter diagram compares total fertility rate and child mortality rate for each country in 1960. The area of each dot represents the population of the country, and the color represents its region of the world. Run the cell. Do you think you can identify any of the dots? In [46]: from functools import lru_cache as cache # This cache annotation makes sure that if the same year # is passed as an argument twice, the work of computing # the result is only carried out once and then saved. @cache ( None ) def stats_relabeled ( year ): \"\"\"Relabeled and cached version of stats_for_year.\"\"\" return stats_for_year ( year ) . relabeled ( 2 , 'Children per woman' ) . relabeled ( 3 , 'Child deaths per 1000 born' ) def fertility_vs_child_mortality ( year ): \"\"\"Draw a color scatter diagram comparing child mortality and fertility.\"\"\" with_region = stats_relabeled ( year ) . join ( 'geo' , countries . select ( 'country' , 'world_6region' ), 'country' ) #with_region adds region to each country with_region . scatter ( 2 , 3 , sizes = 1 , colors = 4 , s = 500 ) #2=fertility, 3=child mortality, sizes=population plots . xlim ( 0 , 10 ) plots . ylim ( - 50 , 500 ) plots . title ( year ) fertility_vs_child_mortality ( 1960 ) The result of the cell below is interactive. It may take several minutes to run because it computers 55 tables (one for each year). When it's done, a scatter plot and a slider should appear. Drag the slider to the right to see how countries have changed over time. You'll find that the great divide between so-called \"Western\" and \"developing\" countries that existed in the 1960's has nearly disappeared. This shift in fertility rates is the reason that the global population is expected to grow more slowly in the 21st century than it did in the 19th and 20th centuries. In [47]: import ipywidgets as widgets # This part takes a few minutes to run because it # computes 55 tables in advance: one for each year. for year in np . arange ( 1960 , 2016 ): stats_relabeled ( year ) _ = widgets . interact ( fertility_vs_child_mortality , year = widgets . IntSlider ( min = 1960 , max = 2015 , value = 1960 )) var element = $('#94aa9764-6740-4325-9412-4bd42f56e295'); {\"model_id\": \"2f317a436f2c4a23b7b8eeaa435c02e9\", \"version_major\": 2, \"version_minor\": 0} Submission Congratulations, you're done with lab 5 and Data 8.1x! Be sure to run all the tests and verify that they all pass (the next cell has a shortcut for that), Review the notebook one last time, we will be grading the final state of your notebook after the deadline , Save and Checkpoint from the File menu, Now is a great time to watch the same data presented by Hans Rosling in a 2010 TEDx talk with smoother animation and witty commentary. Congratulations on finishing Data 8.1X! In [ ]: # For your convenience, you can run this cell to run all the tests at once! import glob from gofer.ok import grade_notebook if not globals () . get ( '__GOFER_GRADER__' , False ): display ( grade_notebook ( 'lab05.ipynb' , sorted ( glob . glob ( 'tests/q*.py' )))) In [ ]:","tags":"World","url":"articles/2018/Nov/Lab5/"},{"title":"Battery Technology: Solid State Electrolyte","text":"This is a Jupyter notebook of physical characteristics of battery chemistries derived from Bill Joy The implementing company is Ionic Materials In [25]: # These lines import the Numpy and Datascience modules. import numpy as np from datascience import * # These lines do some fancy plotting magic. import matplotlib % matplotlib inline import matplotlib.pyplot as plots plots . style . use ( 'fivethirtyeight' ) from ipywidgets import interact , interactive , fixed , interact_manual import ipywidgets as widgets In [1]: 2 ** 3 Out[1]: 8 In [26]: #!pip install datascience #import pandas as pd In [12]: chemistry = pd . read_csv ( 'data/chemistries.csv' ) In [13]: chemistry . head ( 50 ) Out[13]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 Particle size (D50), µm Unnamed: 2 Surface Area (m2/g) Unnamed: 4 Voltage Capacity (mAh/g) Unnamed: 7 Fit to Unnamed: 9 ... Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Unnamed: 39 Unnamed: 40 Unnamed: 41 Unnamed: 42 Unnamed: 43 0 Active Min Max Min Max (V, vs.Li) Theoretical Achieved Graphite NaN ... anode active material kg/Wh cathode active material kg/Wh anode active material cost per kg notes on anode active material cost cathode active material cost per kg notes on cathode active material cost anode active material cost per Wh cathode active material cost per Wh total active material cost per Wh (theoretical... total active material cost per kWh (theoretica... 1 Traditional Cathodes NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0.000969948186528497 0.00259647668393782 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 2 LiCoO2 6.5 9.5 0.4 0.75 3.9 274 155 KS6, SFG6 NaN ... 0.000746113989637306 0.00203108808290155 $22.50 Slide 45 of the March 2015 Avicenne report say... $40.00 Did a search for \"lithium cobalt oxide\" on Ali... $0.016788 $0.081244 $0.098031 $98.031088 3 LiFePO4 2.5 5 NaN 16 3.45 170 160 KS4 NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 4 LiMn2O4 20 25 0.4 1 4.05 148 120 KS15, SFG15 NaN ... 0.00105429150709619 0.0014111286325749 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 5 LiNiCoAlO2 (Ni:Co:Al=8.0:1.5:0.5 5 12 0.3 0.8 3.7 278 200 KS10,SFG10 NaN ... 0.0000435233160621762 0.00108186528497409 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 6 LiNiCoMnO2 (Ni:Co:Mn=5:2:3) 8 12 0.2 0.6 3.8 278 180 KS10,SFG10 NaN ... 0.00110713688784891 0.000782216279458466 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 7 LiNiCoMnO2 (Ni:Co:Mn=1:1:1) 9 12 0.2 0.4 3.8 278 180 KS10,SFG10 NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 8 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 9 High-Voltage Cathodes NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 10 LiNi0.5Mn1.5O4 NaN NaN NaN NaN 4.7 148 130 NaN NaN ... 0.000155793688177108 0.00220336787564767 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 11 LiCoPO4 NaN NaN NaN NaN 4.8 166 NaN NaN NaN ... 0.000115939488875987 0.00133606458609471 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 12 LiNiPO4 NaN NaN NaN NaN 5.1 166 NaN NaN NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 13 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 14 High-Capacity Cathodes NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 15 Sulfur NaN NaN NaN NaN 2.05 1675 NaN NaN NaN ... 0.000174093264248705 0.000743626943005181 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 16 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0 0.000298445595854922 $332.50 Did a search for \"lithium metal foil\" on Aliba... $0.41 Did a search for \"sulfur powder\" on Alibaba. F... $0.043415 $0.000122 $0.043537 $43.536870 17 Anodes NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0 0.00158321749020599 $3.70 Did a search for \"aluminum powder\" on Alibaba.... $2.40 Steve Renter got a quote in March 2014 from a ... $0.000606 $0.003800 $0.004406 $4.405712 18 Lithium metal (Li) NaN NaN NaN NaN 0 3862 NaN NaN NaN ... 0.000172180151454763 0 $3.70 Same as row 19 above. $2.40 Same as row 19 above. $0.000637 $0.001997 $0.002634 $2.634356 19 Artificial graphite NaN 20 NaN 4.075 0.1 372 360 NaN NaN ... 0 0.00162279792746114 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 20 MCMB NaN 17.5 NaN 2.02 0.1 372 360 NaN NaN ... 0.000235614944095991 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 21 Li4Ti5O12 NaN 1.47 1 2 1.55 233 160 KS4, SFG6 NaN ... 0 0.00036139896373057 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 22 Zn metal (Zn) NaN NaN NaN NaN 2.277 825 NaN 7.14 NaN ... 0 0.00125129533678756 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 23 Al metal (Al) NaN NaN NaN NaN 1.364 2978 NaN 2.7 NaN ... 0 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 24 NaN NaN NaN NaN NaN 0.913 2154 1966 NaN NaN ... 0.00101036269430052 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 25 NaN NaN NaN NaN NaN NaN 116 NaN NaN NaN ... 0.000154368411649098 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 26 NaN NaN NaN NaN NaN NaN 1103 NaN NaN NaN ... 0.000176711208071993 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 27 NaN NaN NaN NaN NaN NaN -988 -901.672399993025 NaN NaN ... 0.0000900482401286403 0 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 28 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0.0000705783503710965 0.00190561546001961 $332.50 Same as row 18 above. $40.00 Same as row 4 above. $0.023467 $0.076225 $0.099692 $99.691920 29 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0.000139896373056995 0.00106088082901554 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 30 Map NaN NaN NaN NaN NaN NaN NaN NaN NaN ... 0.000373056994818653 0.00646632124352332 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 31 Battery Wh/Kg Wh/L NaN NaN NaN NaN NaN NaN NaN ... 0.000159881569207994 0.000579570688378979 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 32 NaN NaN Li-Ion MnO2 MnO2 (2e) Li metal Si anode alkaline M/Sulfur MMn2O4 ... 0.000186528497409326 0.00115025906735751 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 33 Zn/MnO2 261 NaN 1533 NaN NaN NaN NaN NaN NaN ... 0.000336592777279988 0.00044879036970665 $2.75 Did a search for magnesium ingot on Alibaba. O... $0.41 Same as row 18 above. $0.000926 $0.000184 $0.001110 $1.109634 34 Zn/MnO2 (2e) 363 NaN NaN 2334 NaN NaN NaN NaN NaN ... 0.000541534347317399 0.000962727728564265 $3.70 Same as row 19 above. $0.41 Same as row 18 above. $0.002004 $0.000395 $0.002398 $2.398395 35 Ni/Zn 431 NaN NaN NaN NaN NaN 1284 NaN NaN ... 0.0000669589477879633 0.00166440813072937 NaN NaN $30.00 Did a search for \"lithium manganese spinel\" on... $0.000000 $0.051941 $0.051941 $51.941012 36 Graphite/LCO 360 1097 NaN NaN NaN NaN NaN NaN NaN ... 0.000233160621761658 0.00101424870466321 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 37 graphite/LIF 373 1092 NaN NaN NaN NaN NaN NaN NaN ... 0.000149222797927461 0.00108186528497409 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 38 LTO/LCO 158 781 NaN NaN NaN NaN NaN NaN NaN ... 0 0.00135233160621762 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 39 LTO/LIF 152 758 NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN $0.000000 40 Si/LTO 424 NaN NaN NaN NaN 1936 NaN NaN NaN ... 0 0.0015419689119171 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 41 Si/LiCoPO4 689 NaN NaN NaN NaN 3155 NaN NaN NaN ... 0 0.00210268487988695 NaN NaN NaN NaN $0.000000 $0.000000 $0.000000 $0.000000 42 Li/MnO2 889 NaN NaN NaN 3308 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 43 Li/FeS2 1090 NaN NaN NaN 1988 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 44 Li/S 2331 NaN NaN NaN NaN NaN NaN 2437 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 45 Mg/S 1273 NaN NaN NaN NaN NaN NaN 2392 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 46 Al/S 665 NaN NaN NaN NaN NaN NaN 1466 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 47 Li/CFx 2257 NaN NaN NaN 2474 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 48 Li/LCO 506 NaN NaN NaN 1887 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 49 Li/LiCoPO4 766 NaN NaN NaN 2785 NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 50 rows × 44 columns In [22]: chemistry . head ( 0 ) Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 Particle size (D50), µm Unnamed: 2 Surface Area (m2/g) Unnamed: 4 Voltage Capacity (mAh/g) Unnamed: 7 Fit to Unnamed: 9 ... Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Unnamed: 39 Unnamed: 40 Unnamed: 41 Unnamed: 42 Unnamed: 43 0 rows × 44 columns In [27]: chemT = Table . read_table ( 'data/chemistries.csv' ) In [28]: chemT Out[28]: Unnamed: 0 Particle size (D50), µm Unnamed: 2 Surface Area (m2/g) Unnamed: 4 Voltage Capacity (mAh/g) Unnamed: 7 Fit to Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 409.786079182631 Unnamed: 16 Unnamed: 17 Unnamed: 18 Unnamed: 19 Unnamed: 20 Unnamed: 21 Unnamed: 22 Unnamed: 23 Unnamed: 24 Unnamed: 25 Unnamed: 26 Unnamed: 27 Unnamed: 28 Li foil thickness Unnamed: 30 Q Unnamed: 32 Unnamed: 33 Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Unnamed: 39 Unnamed: 40 Unnamed: 41 Unnamed: 42 Unnamed: 43 Active Min Max Min Max (V, vs.Li) Theoretical Achieved Graphite nan nan nan nan Battery Reaction M n Ah/kg V Wh/kg achieved Anode Kg Dens kg/L Cathode kg Dens kg/L Volume L Wh/L H2O volume n (H2O) mil cm mAh/cm2 nan kg/Wh anode active material kg/Wh cathode active material kg/Wh anode active material cost per kg notes on anode active material cost cathode active material cost per kg notes on cathode active material cost anode active material cost per Wh cathode active material cost per Wh total active material cost per Wh (theoretical minimum) total active material cost per kWh (theoretical minimum) Traditional Cathodes nan nan nan nan nan nan nan nan nan nan nan nan Zn/MnO2 1/2 Zn + MnO2+1/2H2O 128.5 1 208.6 1.25 261 nan 32.5 7.14 87.0 5.026 21.9 1533 9 0.5 2 0.0050 9.7 nan 0 0.000969948186528497 0.00259647668393782 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiCoO2 6.5 9.5 0.4 0.75 3.9 274 155 KS6, SFG6 nan nan nan nan Graphite/LCO LiC6+2Li0.5CoO2 268 1 100.0 3.6 360 160 72.0 1.5 196.0 4.9 88.0 1097 0 nan 3 0.0075 14.5 nan 0 0.000746113989637306 0.00203108808290155 $22.50 Slide 45 of the March 2015 Avicenne report says that in ... $40.00 Did a search for \"lithium cobalt oxide\" on Alibaba. Firs ... $0.016788 $0.081244 $0.098031 $98.031088 LiFePO4 2.5 5 nan 16 3.45 170 160 KS4 nan nan nan nan Zn/MnO2 (1.33e) 0.67 Zn + MnO2+1/2H2O 139.2 1 256.1 1.2 307 210 43.6 7.14 86.7 5.026 23.3 1833 9 0.5 4 0.0100 19.3 nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiMn2O4 20 25 0.4 1 4.05 148 120 KS15, SFG15 nan nan nan nan Zn/MnO2 (2e) Zn+MnO2+H20 170 2 315.4 1.15 363 nan 65.0 7.14 87.0 5.026 26.4 2334 18 1 5 0.0125 24.1 nan 0 0.00105429150709619 0.0014111286325749 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiNiCoAlO2 (Ni:Co:Al=8.0:1.5:0.5 5 12 0.3 0.8 3.7 278 200 KS10,SFG10 nan nan nan nan Li/MnO2 0.5Li+MnO2 90.5 1 296.2 3.0 889 250 3.5 0.5 87.0 5.026 24.3 3308 0 nan 10 0.0250 48.3 nan 0 0.0000435233160621762 0.00108186528497409 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiNiCoMnO2 (Ni:Co:Mn=5:2:3) 8 12 0.2 0.6 3.8 278 180 KS10,SFG10 nan nan nan nan Ni/Zn NiOOH+Zn+2H2O 193 2 277.8 1.55 431 nan 92.0 7.14 65.0 4.1 64.7 1284 36 2 15 0.0375 72.4 nan 0 0.00110713688784891 0.000782216279458466 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiNiCoMnO2 (Ni:Co:Mn=1:1:1) 9 12 0.2 0.4 3.8 278 180 KS10,SFG10 nan nan nan nan graphite/LIF LiC6+LiFePO4 237 1 113.1 3.3 373 nan 72.0 1.5 165.0 5 81.0 1092 0 nan 20 0.0500 96.6 nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 nan nan nan nan nan nan nan nan nan nan nan nan nan LTO/LCO Li7/3Ti5/3O4+2Li0.5CoO2 356 1 75.2 2.1 158 nan 160.3 5 196.0 4.9 72.1 781 0 nan nan nan nan nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 High-Voltage Cathodes nan nan nan nan nan nan nan nan nan nan nan nan LTO/LIF Li7/3Ti5/3O4+LiFePO4 318 1 84.2 1.8 152 nan 160.3 5 158.0 5 63.7 758 0 nan nan nan nan nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 ... (59 rows omitted) In [29]: chemT = chemT . relabeled ( 'Unnamed: 0' , 'Battery Chemistries' ) chemT Out[29]: Battery Chemistries Particle size (D50), µm Unnamed: 2 Surface Area (m2/g) Unnamed: 4 Voltage Capacity (mAh/g) Unnamed: 7 Fit to Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 409.786079182631 Unnamed: 16 Unnamed: 17 Unnamed: 18 Unnamed: 19 Unnamed: 20 Unnamed: 21 Unnamed: 22 Unnamed: 23 Unnamed: 24 Unnamed: 25 Unnamed: 26 Unnamed: 27 Unnamed: 28 Li foil thickness Unnamed: 30 Q Unnamed: 32 Unnamed: 33 Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Unnamed: 39 Unnamed: 40 Unnamed: 41 Unnamed: 42 Unnamed: 43 Active Min Max Min Max (V, vs.Li) Theoretical Achieved Graphite nan nan nan nan Battery Reaction M n Ah/kg V Wh/kg achieved Anode Kg Dens kg/L Cathode kg Dens kg/L Volume L Wh/L H2O volume n (H2O) mil cm mAh/cm2 nan kg/Wh anode active material kg/Wh cathode active material kg/Wh anode active material cost per kg notes on anode active material cost cathode active material cost per kg notes on cathode active material cost anode active material cost per Wh cathode active material cost per Wh total active material cost per Wh (theoretical minimum) total active material cost per kWh (theoretical minimum) Traditional Cathodes nan nan nan nan nan nan nan nan nan nan nan nan Zn/MnO2 1/2 Zn + MnO2+1/2H2O 128.5 1 208.6 1.25 261 nan 32.5 7.14 87.0 5.026 21.9 1533 9 0.5 2 0.0050 9.7 nan 0 0.000969948186528497 0.00259647668393782 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiCoO2 6.5 9.5 0.4 0.75 3.9 274 155 KS6, SFG6 nan nan nan nan Graphite/LCO LiC6+2Li0.5CoO2 268 1 100.0 3.6 360 160 72.0 1.5 196.0 4.9 88.0 1097 0 nan 3 0.0075 14.5 nan 0 0.000746113989637306 0.00203108808290155 $22.50 Slide 45 of the March 2015 Avicenne report says that in ... $40.00 Did a search for \"lithium cobalt oxide\" on Alibaba. Firs ... $0.016788 $0.081244 $0.098031 $98.031088 LiFePO4 2.5 5 nan 16 3.45 170 160 KS4 nan nan nan nan Zn/MnO2 (1.33e) 0.67 Zn + MnO2+1/2H2O 139.2 1 256.1 1.2 307 210 43.6 7.14 86.7 5.026 23.3 1833 9 0.5 4 0.0100 19.3 nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiMn2O4 20 25 0.4 1 4.05 148 120 KS15, SFG15 nan nan nan nan Zn/MnO2 (2e) Zn+MnO2+H20 170 2 315.4 1.15 363 nan 65.0 7.14 87.0 5.026 26.4 2334 18 1 5 0.0125 24.1 nan 0 0.00105429150709619 0.0014111286325749 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiNiCoAlO2 (Ni:Co:Al=8.0:1.5:0.5 5 12 0.3 0.8 3.7 278 200 KS10,SFG10 nan nan nan nan Li/MnO2 0.5Li+MnO2 90.5 1 296.2 3.0 889 250 3.5 0.5 87.0 5.026 24.3 3308 0 nan 10 0.0250 48.3 nan 0 0.0000435233160621762 0.00108186528497409 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiNiCoMnO2 (Ni:Co:Mn=5:2:3) 8 12 0.2 0.6 3.8 278 180 KS10,SFG10 nan nan nan nan Ni/Zn NiOOH+Zn+2H2O 193 2 277.8 1.55 431 nan 92.0 7.14 65.0 4.1 64.7 1284 36 2 15 0.0375 72.4 nan 0 0.00110713688784891 0.000782216279458466 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 LiNiCoMnO2 (Ni:Co:Mn=1:1:1) 9 12 0.2 0.4 3.8 278 180 KS10,SFG10 nan nan nan nan graphite/LIF LiC6+LiFePO4 237 1 113.1 3.3 373 nan 72.0 1.5 165.0 5 81.0 1092 0 nan 20 0.0500 96.6 nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 nan nan nan nan nan nan nan nan nan nan nan nan nan LTO/LCO Li7/3Ti5/3O4+2Li0.5CoO2 356 1 75.2 2.1 158 nan 160.3 5 196.0 4.9 72.1 781 0 nan nan nan nan nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 High-Voltage Cathodes nan nan nan nan nan nan nan nan nan nan nan nan LTO/LIF Li7/3Ti5/3O4+LiFePO4 318 1 84.2 1.8 152 nan 160.3 5 158.0 5 63.7 758 0 nan nan nan nan nan 0 0 0 nan nan nan nan $0.000000 $0.000000 $0.000000 $0.000000 ... (59 rows omitted) In [ ]:","tags":"Power","url":"articles/2018/Nov/battery-post/"},{"title":"Test","text":"Test Tuesday Markdown","tags":"Power","url":"articles/2018/Nov/test/"},{"title":"Waveglider","text":"Wave Glider SV3 Wave Engine GPCTD+DO Spare Sensor (SKU Code:) Includes a pumped Sea-Bird glider payload conductivity, temperature depth plus dissolved oxygen (GPCTD+DO) sensor with a fairing to reduce drag while it is mounted to the bottom of the sub. Software required for the Sea-Bird GPCTD+DO sensor SV3 Fluorometry Kit (Chyl-A, Trypt, CDOM) (SKU Code:) This sensor kit includes a pumped Turner C3 Fluorometer in a custom foam compartment that occupies 2 Modular Payload Unit (MPU) bays of the Wave Glider SV3 float. The Turner C3 is configured with Chlorophyll A, Tryptophan for Wastewater Monitoring and Chromophoric Dissolved Organic Materials (CDOM) sensors. Teledyne Workhorse Monitor 300kHz Acoustic Doppler Current Profiler (ADCP) Liquid Robotics Wave Height Sensor (SKU Code:) Microstrain IMU, a GPS antenna and proprietary software, the sensor will provide back averaged wave heights, periods and directions SV3 Fluorometry Kit (Chyl-A, Phyco, Refined Fuels) (SKU Code:) This sensor kit includes a pumped Turner C3 Fluorometer in a custom foam compartment that occupies 2 Modular Payload Unit (MPU) bays of the Wave Glider SV3 float. The Turner C3 is configured with Chlorophyll-A, Phycoerythrin (Marine Cyanobacteria) and Refined Fuels sensors.","tags":"Oceans","url":"articles/2018/Nov/waveglider/"},{"title":"North Korea Nuclear Test: Mount Mantap","text":"According to Dreger, the new information suggests the following scenario: The explosion occurred more than a quarter mile (450 meters) below the summit of Mt. Mantap, vaporizing granite rock within a cavity about 160 feet (50 meters) across – the size of a football stadium – and damaging a volume of rock about 1,000 feet (300 meters) across. The blast likely raised the mountain six feet (2 meters) and pushed it outward up to 11 feet (3-4 meters), though within minutes, hours or days the rock above the cavity collapsed to form a depression. Eight and a half minutes after the bomb blast, a nearby underground cavity collapsed, producing the 4.5-magnitude aftershock with the characteristics of an implosion. Subsequently, a much larger volume of fractured rock, perhaps 1 mile (1-2 kilometers) across, compacted, causing the mountain to subside to about 1.5 feet (0.5 meters) lower than before the blast. \"There may be continuing post-explosion compaction at the mountain. It takes time for these aseismic processes to occur,\" Dreger said.","tags":"Nuclear","url":"articles/2018/Nov/Nuclear/"},{"title":"Voting Architecture for the 21st century","text":"Voting architecture: devices, software, the cloud, and integrity Stanford's new center for cybersecurity is in formation. It rests on three pillars: people, system, accountability. for technology, the Computer Science department for management and policy: the Political Science department for educational content: Henry Brady List of technologies by date of introduction point-to-point mass Richard Barnes Washingtonpost.com/wonkblog Stephen Nass quantify the shape of the districts compactness. Compactness score: establish metric Three scores: 22 of them devices. volume/velocity/variety/veracity what frequencies can we see? datification/ connectedness/ networking / authorizing Jstore: Examples of kinds of data: Administrative/ we internet / text/ sensor/audio/video searching distance to polling places in la County uses of N-word in GOOGLE Ken Goldberg pics.....who has opinions about subject: show face animation of branching complexity of voting system; ballot, registration slide of timeline: key applications devices/ show what is inside voting machine; circuit Board Can we use the internet.....show book covers?","tags":"Voting","url":"articles/2018/Sep/voting/"},{"title":"WaterHackathon","text":"Examples: Follow-up: Berkeley Wata Data Collaborative: What happened? Who did what? what are the names of the team members ? .. team members List all open water datasets: who, what, when, where, why State listing of water districts Open and Transparent Water Data Act (AB 1755) Sustainable Groundwater Management Agencies disadvantaged unincorporated communities? small systems to accurately define their service boundaries? means for communities to quickly identify where there may be nearby, available water resources to address a water shortage? annual electronic reporting data collected by the Drinking Water Division of the State Water Resources Control Board? Eileen White, EBMUD Director of WasteWater Water Data Collaborative || --|---|-- Anthony Suen| Violet | 10 students|Weekly meeting","tags":"Water","url":"articles/2018/Sep/waterhackathon-followup/"},{"title":"Dan Kammen at College of Marin","text":"California Environment Data Challenge Saw Felicia Marcus, Matt Rodriguez","tags":"Climate","url":"articles/2018/Sep/COM post/"},{"title":"Tables-test","text":"California Water Data Challenge California Water Data Challenge California Water Data Challenge California Water Data Challenge for water data crunch This is a rst file now...I'm changing. And i'm an inline block quote this is something and this is something and there is some problem with the next line: this is something this is something Header Header what is And all of this and this Section Title1 Section Title2 Section Title3 Section Title4 SectionTitle5 Section Title6 Section Title7 Section Title8 Section Title9 Section Title10 THIS?","tags":"Blogs","url":"articles/2018/Sep/Trying reStructuredText/"},{"title":"California Water Board Hackathon 1","text":"California Water Data Challenge U C Berkeley Hackathon: September; California-wide award ceremony:October Teams gathered at the Berkeley Institute of Data Science to spend two days creating new windows into California's publicly available water data. In preparation, the BIDS team created a list of ten data sets to choose from. Some teams had worked together before; some teams formed, and reformed, in the two days of the Hackathon. By the end of the working sessions, teams presented work in process. Most teams chose a spatial presentation of the data sets, showing values of a variety of water-related parameters for specific areas. Establishing the boundaries of areas: water district service areas, or regional water sheds, or political or administrative districts, proved challenging. For social and demographic data, gathered in US Census Tracts, choosing how to partition the data to fit in water district or political district boundaries required many decisions, and resulted in discovery of many errors in existing spatial boundary definitions.","tags":"Water","url":"articles/2018/Sep/first-post/"},{"title":"Hackathon Teams on Slack","text":"California Water Data Challenge Find names of each team...send notes","tags":"Water","url":"articles/2018/Sep/hackathon/"},{"title":"California Water Board Hackathon 2","text":"California Water Data Challenge Announcement of Water Hackathon caused hundreds of data scientists to create teams to use California data sets to explore water accessibility Evaluating all team entries - beyond first Jupyter Notebooks - data cleaning cost: time, complexity, data dictionary - long-term integration with existing state and local data collection Look at this: Team Topic Dataset 1 2","tags":"Water","url":"articles/2018/Sep/second-post/"},{"title":"California Water Board Hackathon 3 Data Sets","text":"California Water Data Challenge Data Sets Data sets for water data cross-linking project: how to correspond data elements from separate State agencies? A list of state agencies should have footnotes. - California State Water Resources Board - California Water Project List CA professional staff existing Tableau visualizations by professional water board staff existing publications Add something more","tags":"Water","url":"articles/2018/Sep/third-post/"}]}